{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet import DimeNet\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = config['num_features']\n",
    "num_blocks = config['num_blocks']\n",
    "\n",
    "num_bilinear = config['num_bilinear']\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "max_steps = config['max_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "summary_interval = config['summary_interval']\n",
    "validation_interval = config['validation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "targets = config['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(targets)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_folder = best_dir\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "def create_summary(dictionary):\n",
    "    \"\"\"Create a summary from key-value pairs given a dictionary\"\"\"\n",
    "    with summary_writer.as_default():\n",
    "        for key, value in dictionary.items():\n",
    "            tf.summary.scalar(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset, cutoff=cutoff, target_keys=targets)\n",
    "\n",
    "# Initialize DataProvider (splits dataset into training, validation and test set based on data_seed)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                             seed=data_seed, randomized=True)\n",
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "# Initialize datasets\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DimeNet(num_features=num_features, num_blocks=num_blocks, num_bilinear=num_bilinear,\n",
    "                num_spherical=num_spherical, num_radial=num_radial,\n",
    "                cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "                num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "                num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "                activation=swish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load best recorded loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_keys = ['step', 'loss', 'mean_mae', 'mean_log_mae', *targets]\n",
    "best_res = {}\n",
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    for key in save_keys:\n",
    "        best_res[key] = loss_file[key].item()\n",
    "else:\n",
    "    for key in save_keys[1:]:\n",
    "        best_res[key] = np.inf\n",
    "    best_res['step'] = 0\n",
    "    np.savez(best_loss_file, **best_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps,\n",
    "                  decay_steps, decay_rate,\n",
    "                  ema_decay=ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(targets, preds):\n",
    "    \"\"\"Calculate mean absolute error between two values.\"\"\"\n",
    "    delta = tf.abs(targets - preds)\n",
    "    mae = tf.reduce_mean(delta, axis=0)\n",
    "    mean_mae = tf.reduce_mean(mae)\n",
    "    return mean_mae, mae\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(dataset_iter):\n",
    "    inputs, outputs = next(dataset_iter)\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inputs, training=True)\n",
    "        mean_mae, mae = calculate_mae(outputs, preds)\n",
    "        loss = mean_mae\n",
    "    trainer.update_weights(loss, tape)\n",
    "    return loss, mean_mae, mae\n",
    "\n",
    "@tf.function\n",
    "def test_on_batch(dataset_iter):\n",
    "    inputs, outputs = next(dataset_iter)\n",
    "    preds = model(inputs, training=False)\n",
    "    mean_mae, mae = calculate_mae(outputs, preds)\n",
    "    loss = mean_mae\n",
    "    return loss, mean_mae, mae\n",
    "\n",
    "# Initialize training set averages\n",
    "train['num'] = 0\n",
    "train['loss_avg'] = 0.\n",
    "train['mae_avg'] = 0.\n",
    "train['mean_mae_avg'] = 0.\n",
    "\n",
    "def update_average(avg, tmp, num):\n",
    "    \"\"\"Incrementally update an average.\"\"\"\n",
    "    return avg + (tmp - avg) / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "if ckpt_restored is not None:\n",
    "    step = ckpt.step.numpy()\n",
    "else:\n",
    "    step = 0\n",
    "while step <= max_steps:\n",
    "    # Update step number\n",
    "    step += 1\n",
    "    epoch = step // steps_per_epoch\n",
    "    ckpt.step.assign(step)\n",
    "    tf.summary.experimental.set_step(step)\n",
    "\n",
    "    # Perform training step\n",
    "    loss, mean_mae, mae = train_on_batch(train['dataset_iter'])\n",
    "\n",
    "    # Update averages\n",
    "    train['num'] += 1\n",
    "    train['loss_avg'] = update_average(\n",
    "        train['loss_avg'], loss, train['num'])\n",
    "    train['mae_avg'] = update_average(\n",
    "        train['mae_avg'], mae, train['num'])\n",
    "    train['mean_mae_avg'] = update_average(\n",
    "        train['mean_mae_avg'], mean_mae, train['num'])\n",
    "\n",
    "    # Save progress\n",
    "    if (step % save_interval == 0):\n",
    "        manager.save()\n",
    "\n",
    "    # Check performance on the validation set\n",
    "    if (step % validation_interval == 0):\n",
    "        # Save backup variables and load averaged variables\n",
    "        trainer.save_variable_backups()\n",
    "        trainer.load_averaged_variables()\n",
    "\n",
    "        results = {}\n",
    "        if num_valid > 0:\n",
    "            # Initialize validation set averages\n",
    "            validation['num'] = 0\n",
    "            validation['loss_avg'] = 0.\n",
    "            validation['mae_avg'] = 0.\n",
    "            validation['mean_mae_avg'] = 0.\n",
    "\n",
    "            # Compute averages\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "\n",
    "                loss, mean_mae, mae = test_on_batch(validation['dataset_iter'])\n",
    "\n",
    "                validation['num'] += 1\n",
    "                validation['loss_avg'] = update_average(\n",
    "                    validation['loss_avg'], loss, validation['num'])\n",
    "                validation['mae_avg'] = update_average(\n",
    "                    validation['mae_avg'], mae, validation['num'])\n",
    "                validation['mean_mae_avg'] = update_average(\n",
    "                    validation['mean_mae_avg'], mean_mae, validation['num'])\n",
    "\n",
    "            # Store results in dictionary\n",
    "            results['loss_valid'] = validation['loss_avg']\n",
    "            results['mean_mae_valid'] = validation['mean_mae_avg']\n",
    "            results['mean_log_mae_valid'] = np.mean(np.log(validation['mae_avg']))\n",
    "            for i, key in enumerate(targets):\n",
    "                results[key + '_valid'] = validation['mae_avg'][i]\n",
    "\n",
    "            if results[\"mean_mae_valid\"] < best_res['mean_mae']:\n",
    "                best_res['loss'] = results['loss_valid']\n",
    "                best_res['mean_mae'] = results['mean_mae_valid']\n",
    "                best_res['mean_log_mae'] = results['mean_log_mae_valid']\n",
    "                for i, key in enumerate(targets):\n",
    "                    best_res[key] = results[key + '_valid']\n",
    "                best_res['step'] = step\n",
    "\n",
    "                np.savez(best_loss_file, **best_res)\n",
    "                model.save_weights(best_ckpt_folder)\n",
    "\n",
    "        results[\"loss_best\"] = best_res['loss']\n",
    "        results[\"mean_log_mae_best\"] = best_res['mean_log_mae']\n",
    "        create_summary(results)\n",
    "\n",
    "        # Restore backup variables\n",
    "        trainer.restore_variable_backups()\n",
    "\n",
    "    # Generate summaries\n",
    "    if (step % summary_interval == 0) and (step > 0):\n",
    "        results = {}\n",
    "        results['loss_train'] = train['loss_avg']\n",
    "        results['mean_mae_train'] = train['mean_mae_avg']\n",
    "        results['mean_log_mae_train'] = np.mean(np.log(train['mean_mae_avg']))\n",
    "        for i, key in enumerate(targets):\n",
    "            results[key + '_train'] = train['mae_avg'][i]\n",
    "\n",
    "        # Reset training set error averages\n",
    "        train['num'] = 0\n",
    "        train['loss_avg'] = 0.\n",
    "        train['mae_avg'] = 0.\n",
    "        train['mean_mae_avg'] = 0.\n",
    "\n",
    "        create_summary(results)\n",
    "        summary_writer.flush()\n",
    "\n",
    "        logging.info(\n",
    "            f\"{step}/{max_steps} (epoch {epoch+1}): \"\n",
    "            f\"Loss: train={results['loss_train']:.6f}, best={best_res['loss']:.6f}; \"\n",
    "            f\"logMAE: train={results['mean_log_mae_train']:.6f}, best={best_res['mean_log_mae']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
