{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet import DimeNet\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = config['num_features']\n",
    "num_blocks = config['num_blocks']\n",
    "\n",
    "num_bilinear = config['num_bilinear']\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "targets = config['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to load the trained model from\n",
    "directory = # Fill this in\n",
    "\n",
    "log_dir = os.path.join(directory, 'logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset, cutoff=cutoff, target_keys=targets)\n",
    "\n",
    "# Initialize DataProvider (splits dataset into training, validation and test set based on data_seed)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                             seed=data_seed, randomized=True)\n",
    "test = {}\n",
    "\n",
    "# Initialize datasets\n",
    "test['dataset'] = data_provider.get_dataset('test').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test['dataset_iter'] = iter(test['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DimeNet(num_features=num_features, num_blocks=num_blocks, num_bilinear=num_bilinear,\n",
    "                num_spherical=num_spherical, num_radial=num_radial,\n",
    "                cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "                num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "                num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "                activation=swish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(targets, preds):\n",
    "    \"\"\"Calculate mean absolute error between two values.\"\"\"\n",
    "    delta = tf.abs(targets - preds)\n",
    "    mae = tf.reduce_mean(delta, axis=0)\n",
    "    mean_mae = tf.reduce_mean(mae)\n",
    "    return mean_mae, mae\n",
    "\n",
    "@tf.function\n",
    "def test_on_batch(dataset_iter):\n",
    "    inputs, outputs = next(dataset_iter)\n",
    "    preds = model(inputs, training=False)\n",
    "    mean_mae, mae = calculate_mae(outputs, preds)\n",
    "    loss = mean_mae\n",
    "    return loss, mean_mae, mae, preds\n",
    "\n",
    "def update_average(avg, tmp, num):\n",
    "    \"\"\"Incrementally update an average.\"\"\"\n",
    "    return avg + (tmp - avg) / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aggregates\n",
    "loss_avg = 0.\n",
    "mae_avg = 0.\n",
    "mean_mae_avg = 0.\n",
    "preds_total = np.zeros([data_provider.nsamples['test'], len(targets)], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(data_provider.nsamples['test'] / batch_size))\n",
    "for step in range(steps_per_epoch):\n",
    "    \n",
    "    # Perform training step\n",
    "    loss, mean_mae, mae, preds = test_on_batch(test['dataset_iter'])\n",
    "\n",
    "    # Update aggregates\n",
    "    loss_avg = update_average(loss_avg, loss, step + 1)\n",
    "    mae_avg = update_average(mae_avg, mae, step + 1)\n",
    "    mean_mae_avg = update_average(mean_mae_avg, mean_mae, step + 1)\n",
    "    preds_total[step * batch_size:min((step + 1) * batch_size, data_provider.nsamples['test'])] = preds.numpy()\n",
    "mean_log_mae_avg = np.mean(np.log(mae_avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
